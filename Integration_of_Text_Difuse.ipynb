{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMOp/aVp56lCqzxOY4T/Xtr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/codebyrohith/Integration-Of-Text-Difuse/blob/main/Integration_of_Text_Difuse.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxl97vT9IjjX",
        "outputId": "25b67634-4d3e-4763-ff9e-84324e2e4271"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Text-DiFuse'...\n",
            "remote: Enumerating objects: 97, done.\u001b[K\n",
            "remote: Counting objects: 100% (97/97), done.\u001b[K\n",
            "remote: Compressing objects: 100% (96/96), done.\u001b[K\n",
            "remote: Total 97 (delta 7), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (97/97), 4.06 MiB | 8.83 MiB/s, done.\n",
            "Resolving deltas: 100% (7/7), done.\n"
          ]
        }
      ],
      "source": [
        "# Clone Text-DiFuse repo\n",
        "!git clone https://github.com/Leiii-Cao/Text-DiFuse.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd Text-DiFuse/\n",
        "\n",
        "# Install dependencies\n",
        "!pip install -r /content/Text-DiFuse/Text-DiFuse/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zS_DNXAIkz-",
        "outputId": "b0ff219a-9310-4786-865f-905aa5a50b5f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Text-DiFuse\n",
            "Requirement already satisfied: opencv-python-headless==4.8.1.78 in /usr/local/lib/python3.11/dist-packages (from -r /content/Text-DiFuse/Text-DiFuse/requirements.txt (line 1)) (4.8.1.78)\n",
            "Requirement already satisfied: tqdm==4.65.2 in /usr/local/lib/python3.11/dist-packages (from -r /content/Text-DiFuse/Text-DiFuse/requirements.txt (line 2)) (4.65.2)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python-headless==4.8.1.78->-r /content/Text-DiFuse/Text-DiFuse/requirements.txt (line 1)) (1.24.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.24.4\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQxBpHoZQnFE",
        "outputId": "4f4c4ddf-1b8d-4431-dcd0-bfc5ac3910d6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.24.4 in /usr/local/lib/python3.11/dist-packages (1.24.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "from flask import Flask, request, jsonify, send_file\n",
        "import io\n",
        "import torch\n",
        "import argparse\n",
        "import threading\n",
        "import sys\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "# Add your model directory to Python path\n",
        "sys.path.append('/content/Text-DiFuse/Text-DiFuse')\n",
        "\n",
        "from diffusion_fusion.script_util import create_model_and_diffusion, model_and_diffusion_defaults, args_to_dict\n",
        "from diffusion_fusion.unet import Get_Fusion_Control_Model\n",
        "from diffusion_fusion.util import to_numpy_image\n",
        "\n",
        "# Initialize Flask app\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Global model variables\n",
        "diffusion_stage1 = None\n",
        "diffusion_stage2 = None\n",
        "diffusion = None\n",
        "Fusion_Control_Model = None\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def load_model():\n",
        "    global diffusion_stage1, diffusion_stage2, diffusion, Fusion_Control_Model\n",
        "\n",
        "    print(\"üîµ Loading model...\")\n",
        "    defaults = model_and_diffusion_defaults()\n",
        "    args = argparse.Namespace()\n",
        "    for k, v in defaults.items():\n",
        "        setattr(args, k, v)\n",
        "\n",
        "    args.device = str(device)\n",
        "\n",
        "    diffusion_stage1, diffusion_stage2, diffusion = create_model_and_diffusion(\n",
        "        **args_to_dict(args, model_and_diffusion_defaults().keys())\n",
        "    )\n",
        "\n",
        "    Fusion_Control_Model = Get_Fusion_Control_Model()\n",
        "\n",
        "    diffusion_stage1_path = \"/content/diffusion_stage1.pth\"\n",
        "    diffusion_stage2_path = \"/content/diffusion_stage2.pth\"\n",
        "    FCM_path = \"/content/FCM-VIS-IR.pt\"\n",
        "\n",
        "    diffusion_stage1.load_state_dict(torch.load(diffusion_stage1_path, map_location=device))\n",
        "    diffusion_stage2.load_state_dict(torch.load(diffusion_stage2_path, map_location=device))\n",
        "    Fusion_Control_Model.load_state_dict(torch.load(FCM_path, map_location=device), strict=False)\n",
        "\n",
        "    diffusion_stage1 = diffusion_stage1.to(device).eval()\n",
        "    diffusion_stage2 = diffusion_stage2.to(device).eval()\n",
        "    Fusion_Control_Model = Fusion_Control_Model.to(device).eval()\n",
        "\n",
        "    # Free any reserved memory immediately after loading\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.ipc_collect()\n",
        "\n",
        "    print(\"‚úÖ Model loaded and memory cleaned successfully!\")\n",
        "\n",
        "def preprocess_image(img):\n",
        "    img = img.convert('L')  # Convert to grayscale\n",
        "    img = img.resize((512, 512))  # Resize smaller to avoid OOM\n",
        "    img = np.array(img).astype(np.float32) / 255.0\n",
        "    img = torch.from_numpy(img).unsqueeze(0).unsqueeze(0)  # (1,1,H,W)\n",
        "    return img\n",
        "\n",
        "@app.route('/api/fuse', methods=['POST'])\n",
        "def fuse_images():\n",
        "    try:\n",
        "        # Free any unused GPU memory before starting\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.ipc_collect()\n",
        "\n",
        "        vis_image = request.files['vis']\n",
        "        ir_image = request.files['ir']\n",
        "\n",
        "        vis = preprocess_image(Image.open(vis_image))\n",
        "        ir = preprocess_image(Image.open(ir_image))\n",
        "\n",
        "        vis = vis.to(device)\n",
        "        ir = ir.to(device)\n",
        "\n",
        "        cond = {'condition': vis}\n",
        "        cond1 = {'condition': ir}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = diffusion.p_sample_loop(\n",
        "                diffusion_stage1,\n",
        "                diffusion_stage2,\n",
        "                Fusion_Control_Model,\n",
        "                vis.shape,\n",
        "                model_kwargs=cond,\n",
        "                model_kwargs1=cond1,\n",
        "                progress=False,\n",
        "            )\n",
        "\n",
        "        # Clean memory after generation\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.ipc_collect()\n",
        "\n",
        "        output = to_numpy_image(torch.cat((output, vis, ir), dim=1))\n",
        "        output_img = cv2.cvtColor(output[0], cv2.COLOR_YCrCb2RGB)\n",
        "        output_pil = Image.fromarray(output_img)\n",
        "\n",
        "        buf = io.BytesIO()\n",
        "        output_pil.save(buf, format='PNG')\n",
        "        buf.seek(0)\n",
        "\n",
        "        return send_file(buf, mimetype='image/png')\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"‚ùå Error during fusion:\", e)\n",
        "        return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "# Function to run Flask in a separate thread\n",
        "def run_flask():\n",
        "    load_model()\n",
        "    app.run(host='0.0.0.0', port=5000, debug=False, use_reloader=False)\n",
        "\n",
        "# Start Flask server\n",
        "flask_thread = threading.Thread(target=run_flask)\n",
        "flask_thread.daemon = True\n",
        "flask_thread.start()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xPDXAznOhLN",
        "outputId": "deee950a-9b3f-4ab9-c810-db794d148e67"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîµ Loading model...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb\n",
        "!dpkg -i cloudflared-linux-amd64.deb\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yVDnzlTpVSNY",
        "outputId": "cd1aabb6-e534-4756-c06a-56f3241d9f07"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Reading database ... \r(Reading database ... 5%\r(Reading database ... 10%\r(Reading database ... 15%\r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database ... 45%\r(Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 60%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading database ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 126337 files and directories currently installed.)\n",
            "Preparing to unpack cloudflared-linux-amd64.deb ...\n",
            "Unpacking cloudflared (2025.4.0) over (2025.4.0) ...\n",
            "Setting up cloudflared (2025.4.0) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cloudflared tunnel --url http://localhost:5000\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dyFdL7FiQiK",
        "outputId": "4785feb5-22bb-4a51-e31b-52f7d924761f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[90m2025-04-27T20:29:04Z\u001b[0m \u001b[32mINF\u001b[0m Thank you for trying Cloudflare Tunnel. Doing so, without a Cloudflare account, is a quick way to experiment and try it out. However, be aware that these account-less Tunnels have no uptime guarantee, are subject to the Cloudflare Online Services Terms of Use (https://www.cloudflare.com/website-terms/), and Cloudflare reserves the right to investigate your use of Tunnels for violations of such terms. If you intend to use Tunnels in production you should use a pre-created named tunnel by following: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps\n",
            "\u001b[90m2025-04-27T20:29:04Z\u001b[0m \u001b[32mINF\u001b[0m Requesting new quick Tunnel on trycloudflare.com...\n",
            "\u001b[90m2025-04-27T20:29:07Z\u001b[0m \u001b[32mINF\u001b[0m +--------------------------------------------------------------------------------------------+\n",
            "\u001b[90m2025-04-27T20:29:07Z\u001b[0m \u001b[32mINF\u001b[0m |  Your quick Tunnel has been created! Visit it at (it may take some time to be reachable):  |\n",
            "\u001b[90m2025-04-27T20:29:07Z\u001b[0m \u001b[32mINF\u001b[0m |  https://bytes-wu-textbook-remainder.trycloudflare.com                                     |\n",
            "\u001b[90m2025-04-27T20:29:07Z\u001b[0m \u001b[32mINF\u001b[0m +--------------------------------------------------------------------------------------------+\n",
            "\u001b[90m2025-04-27T20:29:07Z\u001b[0m \u001b[32mINF\u001b[0m Cannot determine default configuration path. No file [config.yml config.yaml] in [~/.cloudflared ~/.cloudflare-warp ~/cloudflare-warp /etc/cloudflared /usr/local/etc/cloudflared]\n",
            "\u001b[90m2025-04-27T20:29:07Z\u001b[0m \u001b[32mINF\u001b[0m Version 2025.4.0 (Checksum df13e7e0a027f648c410b5cc701fbcff028724d0e93209796cdbb79ec38695d4)\n",
            "\u001b[90m2025-04-27T20:29:07Z\u001b[0m \u001b[32mINF\u001b[0m GOOS: linux, GOVersion: go1.22.10, GoArch: amd64\n",
            "\u001b[90m2025-04-27T20:29:07Z\u001b[0m \u001b[32mINF\u001b[0m Settings: map[ha-connections:1 protocol:quic url:http://localhost:5000]\n",
            "\u001b[90m2025-04-27T20:29:07Z\u001b[0m \u001b[32mINF\u001b[0m cloudflared will not automatically update if installed by a package manager.\n",
            "\u001b[90m2025-04-27T20:29:07Z\u001b[0m \u001b[32mINF\u001b[0m Generated Connector ID: 710b1b30-11f0-47bf-9d06-620c74269410\n",
            "\u001b[90m2025-04-27T20:29:07Z\u001b[0m \u001b[32mINF\u001b[0m Initial protocol quic\n",
            "\u001b[90m2025-04-27T20:29:07Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "\u001b[90m2025-04-27T20:29:07Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use :: as source for IPv6\n",
            "\u001b[90m2025-04-27T20:29:07Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "\u001b[90m2025-04-27T20:29:07Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use :: as source for IPv6\n",
            "\u001b[90m2025-04-27T20:29:07Z\u001b[0m \u001b[32mINF\u001b[0m Starting metrics server on 127.0.0.1:20241/metrics\n",
            "\u001b[90m2025-04-27T20:29:07Z\u001b[0m \u001b[32mINF\u001b[0m Using [CurveID(4588) CurveID(25497) CurveP256] as curve preferences \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.200.113\n",
            "2025/04/27 20:29:07 failed to sufficiently increase receive buffer size (was: 208 kiB, wanted: 7168 kiB, got: 416 kiB). See https://github.com/quic-go/quic-go/wiki/UDP-Buffer-Sizes for details.\n",
            "\u001b[90m2025-04-27T20:29:07Z\u001b[0m \u001b[32mINF\u001b[0m Registered tunnel connection \u001b[36mconnIndex=\u001b[0m0 \u001b[36mconnection=\u001b[0m3a96508f-b2ce-4880-bb3a-4daabe91d313 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.200.113 \u001b[36mlocation=\u001b[0mlax01 \u001b[36mprotocol=\u001b[0mquic\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [27/Apr/2025 20:31:24] \"POST /api/fuse HTTP/1.1\" 200 -\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "INFO:werkzeug:127.0.0.1 - - [27/Apr/2025 20:45:16] \"POST /api/fuse HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[90m2025-04-27T21:25:17Z\u001b[0m \u001b[32mINF\u001b[0m Initiating graceful shutdown due to signal interrupt ...\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-dF5HnfmiXvz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}